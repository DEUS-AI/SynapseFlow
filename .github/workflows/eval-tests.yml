name: Agent Evaluation Tests

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      category:
        description: 'Filter by category (optional)'
        required: false
        type: string
      severity:
        description: 'Filter by severity (optional)'
        required: false
        type: choice
        options:
          - ''
          - critical
          - high
          - medium
          - low

  # Run on PRs that modify agent code
  pull_request:
    paths:
      - 'src/application/services/**'
      - 'src/application/agents/**'
      - 'tests/eval/**'

  # Scheduled run (e.g., nightly)
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily

env:
  PYTHON_VERSION: '3.13'

jobs:
  # ========================================
  # Unit Tests (no API required)
  # ========================================
  unit-tests:
    name: Evaluator Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run evaluator unit tests
        run: |
          uv run pytest tests/eval/runner/evaluators/ \
            tests/eval/runner/test_scenario_loader.py \
            tests/eval/runner/test_orchestrator_unit.py \
            -v --tb=short

  # ========================================
  # Integration Tests (requires API)
  # ========================================
  integration-tests:
    name: Agent Evaluation Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    # Only run on schedule or manual trigger (not on every PR)
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    services:
      # Neo4j for DIKW graph
      neo4j:
        image: neo4j:5.12
        env:
          NEO4J_AUTH: neo4j/testpassword
          NEO4J_PLUGINS: '["apoc"]'
        ports:
          - 7687:7687
          - 7474:7474
        options: >-
          --health-cmd "wget -q -O- http://localhost:7474"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Redis for session cache
      redis:
        image: redis:7
        ports:
          - 6380:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    env:
      NEO4J_URI: bolt://localhost:7687
      NEO4J_USERNAME: neo4j
      NEO4J_PASSWORD: testpassword
      REDIS_HOST: localhost
      REDIS_PORT: 6380
      SYNAPSEFLOW_EVAL_MODE: 'true'
      SYNAPSEFLOW_EVAL_API_KEY: ${{ secrets.SYNAPSEFLOW_EVAL_API_KEY || 'test-eval-key' }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Wait for services
        run: |
          echo "Waiting for Neo4j..."
          until curl -s http://localhost:7474 > /dev/null; do sleep 2; done
          echo "Neo4j is ready!"

      - name: Start API server
        run: |
          uv run uvicorn src.application.api.main:app \
            --host 0.0.0.0 --port 8000 &
          echo "Waiting for API..."
          sleep 10
          curl -s http://localhost:8000/api/eval/health || echo "API starting..."
          sleep 5

      - name: Run critical scenarios
        run: |
          uv run pytest tests/eval/ \
            --eval-severity=critical \
            --eval-report \
            --eval-report-dir=eval_reports \
            -v --tb=short
        continue-on-error: true

      - name: Run all evaluation scenarios
        if: github.event_name == 'schedule'
        run: |
          CATEGORY="${{ github.event.inputs.category }}"
          SEVERITY="${{ github.event.inputs.severity }}"

          ARGS=""
          if [ -n "$CATEGORY" ]; then
            ARGS="$ARGS --eval-category=$CATEGORY"
          fi
          if [ -n "$SEVERITY" ]; then
            ARGS="$ARGS --eval-severity=$SEVERITY"
          fi

          uv run pytest tests/eval/ \
            $ARGS \
            --eval-report \
            --eval-report-dir=eval_reports \
            -v --tb=short

      - name: Upload evaluation reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: evaluation-reports
          path: eval_reports/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request' && failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '## Agent Evaluation Failed\n\nSome evaluation scenarios failed. Please check the workflow run for details.\n\n[View Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})'
            })

  # ========================================
  # Critical Regression Gate (for PRs)
  # ========================================
  regression-gate:
    name: Critical Regression Check
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Install dependencies
        run: uv sync

      - name: Run critical scenario unit tests (mocked)
        run: |
          uv run pytest tests/eval/runner/ \
            -v --tb=short \
            -k "critical or muriel"

      - name: Validate scenario YAML files
        run: |
          uv run python -c "
          from tests.eval.runner import ScenarioLoader
          from pathlib import Path

          loader = ScenarioLoader(scenarios_dir='tests/eval/scenarios')
          scenarios = loader.load_all_scenarios()
          print(f'Loaded {len(scenarios)} scenarios successfully')

          critical = [s for s in scenarios if s.severity == 'critical']
          print(f'Critical scenarios: {len(critical)}')
          for s in critical:
              print(f'  - {s.id}: {s.name}')
          "
